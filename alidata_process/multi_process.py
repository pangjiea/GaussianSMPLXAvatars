# alidata_process/process.py
# ============================
import json
import os
import shutil
from pathlib import Path
import numpy as np
import math
from tqdm import tqdm
from copy import deepcopy
import cv2  # ç”¨äºå¤„ç†Rodrigueså‘é‡å’Œå›¾åƒå»ç•¸å˜
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import threading

# --- é…ç½®åŒºåŸŸ ---
alidata_path = Path("/home/hello/data")
subject_name = "SC_01"

MASK_BASE_DIR = Path("/home/hello/remote/server2/data1/sapiens_processing/final_head_segments_multigpu/raw_videos/masks")
IMAGE_BASE_DIR = Path("/home/hello/remote/server2/data1/sapiens_processing/final_head_segments_multigpu/raw_videos/masked_images")  # åŸå§‹å›¾åƒç›®å½•
UNDISTORTED_IMAGE_DIR = Path("/home/hello/remote/server2/data1/sapiens_processing/alidata_process/undistorted_images") / subject_name
SMPLX_FITTING_DIR = alidata_path / subject_name / "smplx_fitting"
CALIBRATION_FILE = alidata_path / subject_name / "calibration.json"
OUTPUT_DATASET_DIR = Path("/home/hello/data/SC_01/export_all")

NUM_FRAMES = 1280  # æ€»å¸§æ•°
# ç”Ÿæˆ001åˆ°053çš„ç›¸æœºIDåˆ—è¡¨ï¼Œæ’é™¤æŒ‡å®šçš„æ•°å­—
excluded_numbers = {"002", "006", "012", "021", "024", "025", "030", "033", "043", "044", "051"}
USED_CAMERA_ID_STR_LIST = [f"{i:03d}" for i in range(1, 54) if f"{i:03d}" not in excluded_numbers]
TEST_CAMERA_ID_STR_LIST = ["046"]  # æµ‹è¯•é›†ç›¸æœºID
TRAIN_RATIO = 0.7  # è®­ç»ƒé›†æ—¶é—´æ­¥æ¯”ä¾‹
TRAIN_VAL_SUBJECT_SEED = "SC_01"

# æ€§èƒ½ä¼˜åŒ–é…ç½® - é’ˆå¯¹16æ ¸ç³»ç»Ÿä¼˜åŒ–
MAX_WORKERS_MULTIPLIER = 6  # I/Oå¯†é›†å‹ä»»åŠ¡çš„çº¿ç¨‹å€æ•°ï¼ˆ16æ ¸ Ã— 6 = 96çº¿ç¨‹ï¼‰
BATCH_SIZE = 100  # æ‰¹å¤„ç†å¤§å°ï¼Œ16æ ¸ç³»ç»Ÿå¯ä»¥å¤„ç†æ›´å¤§æ‰¹æ¬¡
PROGRESS_UPDATE_INTERVAL = 50  # è¿›åº¦æ›´æ–°é—´éš”ï¼Œå‡å°‘é¢‘ç¹æ›´æ–°çš„å¼€é”€

# å…¨å±€å˜é‡ï¼Œç”¨äºå¹¶è¡Œè®¿é—®ï¼ˆé‡å‘½åä»¥åæ˜ OpenCVåæ ‡ç³»ï¼‰
all_camera_params_opencv = {}
cam_id_str_to_int_idx = {}

# --- è¾…åŠ©å‡½æ•° ---
def ensure_dir(path: Path):
    path.mkdir(parents=True, exist_ok=True)


def write_json_to_file(data, filepath: Path, indent=4):
    print(f"æ­£åœ¨å†™å…¥ JSON åˆ°: {filepath}")
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=indent, ensure_ascii=False)


def load_smplx_params(json_path: Path):
    with open(json_path, 'r') as f:
        return json.load(f)


def save_smplx_params(params, json_path: Path):
    ensure_dir(json_path.parent)
    for key, value in params.items():
        if isinstance(value, np.ndarray):
            params[key] = value.tolist()
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(params, f, indent=4)


def create_canonical_smplx_params(first_frame_smplx_path: Path, output_path: Path):
    print(f"æ­£åœ¨ä» {first_frame_smplx_path} åˆ›å»ºæ ‡å‡†SMPL-Xå‚æ•° (canonical_smplx_param.json)")
    params = load_smplx_params(first_frame_smplx_path)
    canonical_params = {}
    # betas or shape
    if 'betas' in params:
        canonical_params['betas'] = params['betas']
    elif 'shape' in params:
        canonical_params['betas'] = params['shape']
    else:
        print("è­¦å‘Š: æœªæ‰¾åˆ° 'betas' æˆ– 'shape'ï¼Œä½¿ç”¨é›¶å‘é‡ã€‚")
        canonical_params['betas'] = np.zeros(100).tolist()
    # å…¶ä»–å‚æ•°ç½®é›¶
    param_zeros_configs = {
        'global_orient': 3, 'body_pose': 63, 'jaw_pose': 3, 'leye_pose': 3,
        'reye_pose': 3, 'expression': 50, 'transl': 3,
        'left_hand_pose': 45, 'right_hand_pose': 45,
        'Rh': 3, 'Th': 3
    }
    for key, default_size in param_zeros_configs.items():
        if key in params:
            arr = np.array(params[key])
            # å¦‚æœåµŒå¥—åˆ—è¡¨
            if arr.ndim > 1:
                canonical_params[key] = [np.zeros_like(sub).tolist() for sub in arr]
            else:
                canonical_params[key] = np.zeros_like(arr).tolist()
        else:
            canonical_params[key] = np.zeros(default_size).tolist()
            print(f"ä¿¡æ¯: å‚æ•° '{key}' æœªåœ¨æºæ–‡ä»¶ä¸­æ‰¾åˆ°ï¼Œå·²æ·»åŠ é›¶å€¼ã€‚")
    canonical_params['gender'] = params.get('gender', 'neutral')
    save_smplx_params(canonical_params, output_path)
    print(f"æ ‡å‡†SMPL-Xå‚æ•°å·²ä¿å­˜åˆ°: {output_path}")
    return canonical_params


def process_calibration_data(calib_file: Path, specified_camera_ids: list = None):
    data = json.load(open(calib_file, 'r'))
    cameras = data.get('cameras', {})
    poses = data.get('camera_poses', {})
    
    if specified_camera_ids:
        ids = [cid for cid in specified_camera_ids if cid in cameras and cid in poses]
        missing = set(specified_camera_ids) - set(ids)
        if missing:
            print(f"è­¦å‘Š: ç›¸æœºID æœªæ‰¾åˆ°: {missing}")
    else:
        ids = [cid for cid in cameras if cid in poses]
    
    if not ids:
        raise ValueError("æ²¡æœ‰å¯ç”¨çš„ç›¸æœºæ•°æ®")
    
    out = {}
    for cid in tqdm(ids, desc="å¤„ç†ç›¸æœºæ ‡å®š"):
        intr = cameras[cid]
        w, h = intr['image_size']
        K = np.array(intr['K'])
        dist = np.array(intr.get('dist', np.zeros(5))).flatten()
        
        fx, fy = K[0,0], K[1,1]
        angle_x = 2 * math.atan(w / (2 * fx))
        angle_y = 2 * math.atan(h / (2 * fy))
        
        p = poses[cid]
        R = np.array(p['R'])
        T = np.array(p['T']).flatten()
        w2c = np.eye(4)
        w2c[:3,:3] = R
        w2c[:3,3] = T
        c2w = np.linalg.inv(w2c)   
        c2w_final = c2w

        cx, cy = K[0,2], K[1,2]
        out[cid] = {
            'id': int(cid),
            'width': w,
            'height': h,
            'camera_angle_x': angle_x,
            'camera_angle_y': angle_y,
            'fl_x': fx,
            'fl_y': fy,
            'cx': cx,
            'cy': cy,
            'transform_matrix': c2w_final.tolist(),
            
            # åŸå§‹å‚æ•°ï¼ˆç”¨äºå»ç•¸å˜å¤„ç†ï¼‰
            'original_K_cv': K.tolist(),
            'original_dist_cv': dist.tolist(),
            
            # å»ç•¸å˜åçš„å‚æ•°ï¼ˆç”¨äºåç»­æ¸²æŸ“ï¼‰
            'undistorted_K_cv': K.tolist(),  # KçŸ©é˜µä¿æŒä¸å˜
            'undistorted_dist_cv': [0.0, 0.0, 0.0, 0.0, 0.0],  # ç•¸å˜ç³»æ•°ç½®é›¶
            
            # æ ‡è®°å›¾åƒå·²å»ç•¸å˜å’Œä¸»ç‚¹ä¿¡æ¯
            'image_undistorted': True,
        }
    
    print(f"âœ… æˆåŠŸå¤„ç† {len(out)} ä¸ªç›¸æœºï¼Œä½¿ç”¨OpenCVåæ ‡ç³»")
    return out


def process_single_frame_cam(frame_idx, cam_id_str):
    global all_camera_params_opencv, cam_id_str_to_int_idx

    try:
        # å¿«é€Ÿæ£€æŸ¥ï¼šç¡®ä¿ç›¸æœºå‚æ•°å­˜åœ¨
        if cam_id_str not in all_camera_params_opencv:
            return {"error": f"ç›¸æœºå‚æ•°ä¸å­˜åœ¨: {cam_id_str}"}

        # å¿«é€Ÿæ£€æŸ¥ï¼šSMPL-X å‚æ•°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        src = SMPLX_FITTING_DIR/f"{frame_idx:06d}.json"
        if not src.exists():
            return {"error": f"SMPLXæ–‡ä»¶ä¸å­˜åœ¨: {src}"}

        # å¿«é€Ÿæ£€æŸ¥ï¼šå›¾åƒå’Œmaskæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        stem = f"{frame_idx:06d}_{cam_id_str}"
        img_p = IMAGE_BASE_DIR/f"{stem}.png"
        if not img_p.exists():
            img_p = IMAGE_BASE_DIR/f"{stem}.jpg"
        mask_p = MASK_BASE_DIR/f"{stem}.png"

        if not img_p.exists():
            return {"error": f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img_p} å’Œ {IMAGE_BASE_DIR/f'{stem}.jpg'}"}
        if not mask_p.exists():
            return {"error": f"æ©ç æ–‡ä»¶ä¸å­˜åœ¨: {mask_p}"}

        # è·å–ç›¸æœºå‚æ•°
        cam = all_camera_params_opencv[cam_id_str]
        idx = cam_id_str_to_int_idx[cam_id_str]

        # é¢„å…ˆè®¡ç®—è¾“å‡ºè·¯å¾„
        rel = Path('smplx_param')/src.name
        name = f"{frame_idx:06d}_{idx:02d}.png"
        output_img_path = OUTPUT_DATASET_DIR/'images'/name
        output_mask_path = OUTPUT_DATASET_DIR/'masks_images'/name
        output_smplx_path = OUTPUT_DATASET_DIR/rel

        # å¤åˆ¶SMPL-Xå‚æ•°ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡å¤„ç†è¯¥å¸§æ—¶å¤åˆ¶ï¼‰
        if not output_smplx_path.exists():
            shutil.copyfile(str(src), str(output_smplx_path))

        # è¯»å–å›¾åƒå’Œmask
        img = cv2.imread(str(img_p))
        mask = cv2.imread(str(mask_p), cv2.IMREAD_GRAYSCALE)

        if img is None or mask is None:
            return None

        # å»ç•¸å˜å¤„ç†ï¼ˆä½¿ç”¨é¢„å…ˆè½¬æ¢çš„numpyæ•°ç»„ï¼‰
        K_original = np.array(cam['original_K_cv'], dtype=np.float64)
        dist_original = np.array(cam['original_dist_cv'], dtype=np.float64)

        # å¹¶è¡Œå»ç•¸å˜å¤„ç†
        img_undistorted = cv2.undistort(img, K_original, dist_original, None, K_original)
        mask_undistorted = cv2.undistort(mask, K_original, dist_original, None, K_original)

        # ä¿å­˜å¤„ç†åçš„å›¾åƒå’Œmask
        cv2.imwrite(str(output_img_path), img_undistorted)
        cv2.imwrite(str(output_mask_path), mask_undistorted)

        # è¿”å›å…ƒæ•°æ®ï¼ˆé¢„å…ˆæ„å»ºå­—å…¸ä»¥å‡å°‘é‡å¤è®¡ç®—ï¼‰
        return {
            "timestep_index": frame_idx,
            "timestep_index_original": frame_idx,
            "camera_index": idx,
            "cx": cam['cx'],
            "cy": cam['cy'],
            "fl_x": cam['fl_x'],
            "fl_y": cam['fl_y'],
            "h": cam['height'],
            "w": cam['width'],
            "camera_angle_x": cam['camera_angle_x'],
            "camera_angle_y": cam['camera_angle_y'],
            "transform_matrix": cam['transform_matrix'],
            "file_path": str(Path('images')/name),
            "fg_mask_path": str(Path('masks_images')/name),
            "smplx_param_path": str(rel),
        }

    except Exception as e:
        # è¿”å›é”™è¯¯ä¿¡æ¯ç”¨äºè¯Šæ–­
        return {"error": f"å¤„ç†å¼‚å¸¸: {str(e)}"}


def setup_directories():
    """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¾“å‡ºç›®å½•ã€‚"""
    print("ğŸ“‚ åˆ›å»ºè¾“å‡ºç›®å½•...")
    for p in [OUTPUT_DATASET_DIR, OUTPUT_DATASET_DIR/'images', OUTPUT_DATASET_DIR/'masks_images', OUTPUT_DATASET_DIR/'smplx_param', UNDISTORTED_IMAGE_DIR]:
        ensure_dir(p)
    print("âœ… ç›®å½•åˆ›å»ºå®Œæˆã€‚")

def load_and_process_calibration():
    """åŠ è½½å¹¶å¤„ç†ç›¸æœºæ ‡å®šæ•°æ®ã€‚"""
    try:
        camera_params = process_calibration_data(CALIBRATION_FILE, USED_CAMERA_ID_STR_LIST)
        sorted_ids = sorted(camera_params.keys(), key=int)
        cam_id_map = {cid: i for i, cid in enumerate(sorted_ids)}
        return camera_params, cam_id_map, sorted_ids
    except Exception as e:
        print(f"âŒ æ ‡å®šå¤„ç†å¤±è´¥: {e}")
        return None, None, None

def create_canonical_smplx_parameters():
    """åˆ›å»ºæ ‡å‡†SMPL-Xå‚æ•°ã€‚"""
    first_frame_smplx_path = SMPLX_FITTING_DIR / '000009.json'
    if not first_frame_smplx_path.exists():
        print("âŒ ç¼ºå°‘ç¬¬ä¸€å¸§SMPL-Xå‚æ•°æ–‡ä»¶: 000009.json")
        return False
    create_canonical_smplx_params(first_frame_smplx_path, OUTPUT_DATASET_DIR / 'canonical_smplx_param.json')
    return True

def process_all_frames_and_cameras(camera_params, cam_id_map, sorted_camera_ids):
    """å¹¶è¡Œå¤„ç†æ‰€æœ‰å¸§å’Œç›¸æœºï¼ŒåŒ…æ‹¬SMPL-Xå‚æ•°å¤åˆ¶ã€å›¾åƒå»ç•¸å˜å’Œæ©ç å¤„ç†ã€‚"""
    global all_camera_params_opencv, cam_id_str_to_int_idx
    all_camera_params_opencv = camera_params
    cam_id_str_to_int_idx = cam_id_map

    # è®¡ç®—è·³è·ƒè¯»å–çš„å¸§æ•°å’Œæ­¥é•¿
    total_frames = NUM_FRAMES
    num_frames_to_read = max(1, int(total_frames * 0.2))  # å‡åˆ†è¯»å–å…¶ä¸­çš„20%
    step = max(1, int(total_frames / num_frames_to_read))

    print(f"æ€»å¸§æ•°: {total_frames}, å°†è·³è·ƒè¯»å– {num_frames_to_read} å¸§ï¼Œæ­¥é•¿ä¸º {step}")

    # ç”Ÿæˆæ‰€æœ‰ä»»åŠ¡å‰å…ˆæ£€æŸ¥æ–‡ä»¶å­˜åœ¨æƒ…å†µ
    available_frames = [f for f in range(0, total_frames, step) if (SMPLX_FITTING_DIR / f"{f:06d}.json").exists()]
    print(f"å¯ç”¨å¸§æ•°: {len(available_frames)}/{num_frames_to_read}")

    # æ£€æŸ¥å›¾åƒæ–‡ä»¶å­˜åœ¨æƒ…å†µ
    sample_frame = available_frames[0] if available_frames else 0
    sample_cam = sorted_camera_ids[0] if sorted_camera_ids else "001"
    sample_stem = f"{sample_frame:06d}_{sample_cam}"
    sample_img = IMAGE_BASE_DIR / f"{sample_stem}.png"
    sample_mask = MASK_BASE_DIR / f"{sample_stem}.png"

    print(f"ğŸ“ æ£€æŸ¥æ–‡ä»¶è·¯å¾„:")
    print(f"   SMPLXç›®å½•: {SMPLX_FITTING_DIR}")
    print(f"   å›¾åƒç›®å½•: {IMAGE_BASE_DIR}")
    print(f"   æ©ç ç›®å½•: {MASK_BASE_DIR}")
    print(f"   æ ·ä¾‹æ–‡ä»¶: {sample_img} (å­˜åœ¨: {sample_img.exists()})")
    print(f"   æ ·ä¾‹æ©ç : {sample_mask} (å­˜åœ¨: {sample_mask.exists()})")

    tasks = [(f, c) for f in available_frames for c in sorted_camera_ids]
    print(f"æ€»ä»»åŠ¡æ•°: {len(tasks)} (å¸§æ•° Ã— ç›¸æœºæ•°)")

    # ä¼˜åŒ–å¹¶è¡Œå¤„ç†ï¼šæ ¹æ®å®é™…CPUæ ¸å¿ƒæ•°è°ƒæ•´
    actual_cores = os.cpu_count()
    # å¦‚æœæ£€æµ‹åˆ°çš„æ ¸å¿ƒæ•°å¼‚å¸¸ï¼Œä½¿ç”¨ä¿å®ˆä¼°è®¡
    if actual_cores > 20:  # å¯èƒ½æ˜¯è¶…çº¿ç¨‹å¯¼è‡´çš„
        effective_cores = actual_cores // 2
        print(f"âš ï¸  æ£€æµ‹åˆ° {actual_cores} ä¸ªé€»è¾‘æ ¸å¿ƒï¼Œå¯èƒ½åŒ…å«è¶…çº¿ç¨‹ï¼Œä½¿ç”¨ {effective_cores} ä¸ªç‰©ç†æ ¸å¿ƒè®¡ç®—")
    else:
        effective_cores = actual_cores

    max_workers = min(len(tasks), effective_cores * MAX_WORKERS_MULTIPLIER)
    print(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†")
    print(f"   é€»è¾‘æ ¸å¿ƒæ•°: {actual_cores}, æœ‰æ•ˆæ ¸å¿ƒæ•°: {effective_cores}, çº¿ç¨‹å€æ•°: {MAX_WORKERS_MULTIPLIER}")

    frames_all = []
    error_stats = {}
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æ‰¹é‡æäº¤ä»»åŠ¡ä»¥å‡å°‘å¼€é”€
        futures = {executor.submit(process_single_frame_cam, f, c): (f, c) for f, c in tasks}

        # ä½¿ç”¨æ›´é«˜æ•ˆçš„ç»“æœæ”¶é›†æ–¹å¼
        completed_count = 0

        with tqdm(total=len(futures), desc="å¹¶è¡Œå¯¼å‡ºå¸§æ•°æ®", unit="ä»»åŠ¡") as pbar:
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result:
                        if "error" in result:
                            # æ”¶é›†é”™è¯¯ç»Ÿè®¡
                            error_type = result["error"].split(":")[0]
                            error_stats[error_type] = error_stats.get(error_type, 0) + 1
                        else:
                            frames_all.append(result)
                    completed_count += 1
                    pbar.update(1)

                    # å®šæœŸæ›´æ–°è¿›åº¦ä¿¡æ¯å’Œæ€§èƒ½ç»Ÿè®¡
                    if completed_count % PROGRESS_UPDATE_INTERVAL == 0:
                        current_time = time.time()
                        elapsed = current_time - start_time
                        speed = completed_count / elapsed if elapsed > 0 else 0
                        success_rate = len(frames_all) / completed_count * 100 if completed_count > 0 else 0
                        eta = (len(futures) - completed_count) / speed if speed > 0 else 0

                        pbar.set_postfix({
                            "æˆåŠŸ": len(frames_all),
                            "æˆåŠŸç‡": f"{success_rate:.1f}%",
                            "é€Ÿåº¦": f"{speed:.1f}ä»»åŠ¡/ç§’",
                            "é¢„è®¡å‰©ä½™": f"{eta/60:.1f}åˆ†é’Ÿ"
                        })

                        # å¦‚æœæˆåŠŸç‡å¤ªä½ï¼Œæ˜¾ç¤ºé”™è¯¯ç»Ÿè®¡
                        if completed_count >= 100 and success_rate < 10:
                            print(f"\nâš ï¸  æˆåŠŸç‡è¿‡ä½ ({success_rate:.1f}%)ï¼Œé”™è¯¯ç»Ÿè®¡:")
                            for error_type, count in error_stats.items():
                                print(f"   {error_type}: {count} æ¬¡")

                except Exception as e:
                    print(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                    pbar.update(1)

    end_time = time.time()
    total_time = end_time - start_time
    avg_speed = len(tasks) / total_time if total_time > 0 else 0

    print(f"âœ… å¹¶è¡Œå¤„ç†å®Œæˆ: æˆåŠŸå¤„ç† {len(frames_all)}/{len(tasks)} ä¸ªä»»åŠ¡")
    print(f"   æ€»è€—æ—¶: {total_time/60:.2f} åˆ†é’Ÿ")
    print(f"   å¹³å‡é€Ÿåº¦: {avg_speed:.2f} ä»»åŠ¡/ç§’")
    print(f"   æˆåŠŸç‡: {len(frames_all)/len(tasks)*100:.1f}%")

    # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ç»Ÿè®¡
    if error_stats:
        print(f"\nğŸ“Š é”™è¯¯ç»Ÿè®¡:")
        for error_type, count in sorted(error_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(tasks) * 100
            print(f"   {error_type}: {count} æ¬¡ ({percentage:.1f}%)")

    return frames_all

def generate_transforms_metadata(frames_data, camera_params, sorted_camera_ids):
    """ç”Ÿæˆtransforms.jsonæ–‡ä»¶çš„å…ƒæ•°æ®ã€‚"""
    meta = {
        'camera_angle_x': 0, 'camera_angle_y': 0, 'fl_x': 0, 'fl_y': 0, 'cx': 0, 'cy': 0, 'w': 0, 'h': 0,
        'frames': [],'applied_transform': np.eye(4).tolist(),
        'k1': 0, 'k2': 0, 'p1': 0, 'p2': 0, 'k3': 0
    }

    if sorted_camera_ids:
        first_cam_id = sorted_camera_ids[0]
        first_cam_data = camera_params[first_cam_id]
        meta.update({
            'camera_angle_x': first_cam_data['camera_angle_x'],
            'camera_angle_y': first_cam_data['camera_angle_y'],
            'fl_x': first_cam_data['fl_x'],
            'fl_y': first_cam_data['fl_y'],
            'cx': first_cam_data['cx'],
            'cy': first_cam_data['cy'],
            'w': first_cam_data['width'],
            'h': first_cam_data['height'],
            'k1': 0.0, 'k2': 0.0, 'k3': 0.0,
            'p1': 0.0, 'p2': 0.0,
            'images_undistorted': True,
            'distortion_model': 'none',
        })
    else:
        print("è­¦å‘Šï¼šæ²¡æœ‰å¯ç”¨çš„ç›¸æœºå‚æ•°æ¥åˆå§‹åŒ–metaå­—å…¸ã€‚")
        
    meta['frames'] = frames_data
    if frames_data:
        meta['timestep_indices'] = sorted(list(set(f['timestep_index'] for f in frames_data)))
        meta['camera_indices'] = sorted(list(set(f['camera_index'] for f in frames_data)))
    else:
        meta['timestep_indices'] = []
        meta['camera_indices'] = []
    
    write_json_to_file(meta, OUTPUT_DATASET_DIR / 'transforms.json')
    return meta

def split_dataset_and_save_metadata(frames_data, base_meta, cam_id_map):
    """åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†å¹¶ä¿å­˜ç›¸åº”çš„transforms_*.jsonæ–‡ä»¶ã€‚é‡‡ç”¨åŸç‰ˆé€»è¾‘ã€‚"""
    print("ğŸ“Š åˆ’åˆ† train/val/test æ•°æ®é›†...")

    # è·å–æ‰€æœ‰æ—¶é—´æ­¥å¹¶æŒ‰åŸç‰ˆé€»è¾‘åˆ’åˆ†
    all_timesteps = sorted(list(set(f['timestep_index'] for f in frames_data)))
    nt = len(all_timesteps)
    assert 0 < TRAIN_RATIO <= 1
    nt_train = int(np.ceil(nt * TRAIN_RATIO))

    # æ—¶é—´æ­¥åˆ’åˆ†ï¼šå‰70%ç”¨äºè®­ç»ƒ+éªŒè¯ï¼Œå30%ç”¨äºæµ‹è¯•
    train_val_timesteps = all_timesteps[:nt_train]
    test_timesteps = all_timesteps[nt_train:]

    # ç›¸æœºåˆ’åˆ†
    all_camera_indices = sorted(list(set(f['camera_index'] for f in frames_data)))
    test_camera_int_ids = {cam_id_map[c] for c in TEST_CAMERA_ID_STR_LIST if c in cam_id_map}

    if test_camera_int_ids:
        # æœ‰æŒ‡å®šæµ‹è¯•ç›¸æœºçš„æƒ…å†µ
        train_camera_indices = [c for c in all_camera_indices if c not in test_camera_int_ids]
        val_camera_indices = list(test_camera_int_ids)  # éªŒè¯é›†ä½¿ç”¨æµ‹è¯•ç›¸æœº
        test_camera_indices = all_camera_indices  # æµ‹è¯•é›†ä½¿ç”¨æ‰€æœ‰ç›¸æœº
    else:
        # æ²¡æœ‰æŒ‡å®šæµ‹è¯•ç›¸æœºï¼Œä½¿ç”¨æœ€åä¸€ä¸ªç›¸æœºä½œä¸ºéªŒè¯ç›¸æœº
        train_camera_indices = all_camera_indices[:-1] if len(all_camera_indices) > 1 else all_camera_indices
        val_camera_indices = [all_camera_indices[-1]] if len(all_camera_indices) > 1 else []
        test_camera_indices = all_camera_indices

    # æŒ‰åŸç‰ˆé€»è¾‘åˆ†é…å¸§æ•°æ®
    train_set = []
    val_set = []
    test_set = []

    for frame in frames_data:
        timestep = frame['timestep_index']
        camera_idx = frame['camera_index']

        if timestep in train_val_timesteps:
            # è®­ç»ƒ+éªŒè¯æ—¶é—´æ®µ
            if camera_idx in train_camera_indices:
                train_set.append(frame)
            elif camera_idx in val_camera_indices:
                val_set.append(frame)
        elif timestep in test_timesteps:
            # æµ‹è¯•æ—¶é—´æ®µï¼Œæ‰€æœ‰ç›¸æœºéƒ½è¿›å…¥æµ‹è¯•é›†
            test_set.append(frame)

    def save_split_metadata(name, data_subset, timestep_list, camera_list):
        split_meta = deepcopy(base_meta)
        split_meta['frames'] = data_subset
        split_meta['timestep_indices'] = timestep_list
        split_meta['camera_indices'] = camera_list
        write_json_to_file(split_meta, OUTPUT_DATASET_DIR / f'transforms_{name}.json')

    save_split_metadata('train', train_set, train_val_timesteps, train_camera_indices)
    save_split_metadata('val', val_set, train_val_timesteps, val_camera_indices)  # éªŒè¯é›†å…±äº«è®­ç»ƒé›†æ—¶é—´æ­¥
    save_split_metadata('test', test_set, test_timesteps, test_camera_indices)

    print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›†={len(train_set)}, éªŒè¯é›†={len(val_set)}, æµ‹è¯•é›†={len(test_set)}")
    print(f"   æ—¶é—´æ­¥åˆ’åˆ†: è®­ç»ƒ+éªŒè¯={len(train_val_timesteps)}, æµ‹è¯•={len(test_timesteps)}")
    print(f"   ç›¸æœºåˆ’åˆ†: è®­ç»ƒ={train_camera_indices}, éªŒè¯={val_camera_indices}, æµ‹è¯•={test_camera_indices}")

def main():
    setup_directories()

    camera_params, cam_id_map, sorted_camera_ids = load_and_process_calibration()
    if camera_params is None:
        return

    if not create_canonical_smplx_parameters():
        return

    frames_data = process_all_frames_and_cameras(camera_params, cam_id_map, sorted_camera_ids)
    
    base_meta = generate_transforms_metadata(frames_data, camera_params, sorted_camera_ids)
    
    split_dataset_and_save_metadata(frames_data, base_meta, cam_id_map)

if __name__ == '__main__':
    main()
